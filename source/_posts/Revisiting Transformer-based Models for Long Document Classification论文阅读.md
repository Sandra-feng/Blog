---
title: 文本分类综述论文阅读笔记
categories:
  - 阅读笔记
tags:
  - 学习记录
toc: true
date: 2023-07-28 09:33
updated: 2023-07-28 09:33s
comments: true
---

# Revisiting Transformer-based Models for Long Document Classification论文阅读

## 摘要

比较了不同的基于transformer的长文档分类(TrLDC)方法，这些方法旨在减轻普通转换器编码更长的文本的计算开销，即稀疏注意和分层编码方法。我们在覆盖不同领域的四个文档分类数据集上研究了稀疏注意(例如，局部注意窗口的大小，全局注意的使用)和分层(例如，文档分割策略)转换的几个方面。我们观察到能够处理较长的文本的明显好处，并且根据我们的结果，我们得出了在长文档分类任务上应用基于transformer的模型的实用建议。

## 引言

对于长文档分类来说，截断文本可能会遗漏重要信息，导致分类性能差(图1)。另一个挑战就是每一个token都会关注其他所有token，在多头自注意力的操作中计算开销。使得处理长文本变得非常困难。

对于第二个挑战，长文本的transformer已经出现。但是他们的实验在一个并不好的数据集上。在一些数据集上，BERT的多个变体比基于CNN或rnn的模型表现更差。作者们认为有必要了解基于transformer的模型在对实际较长的文档进行分类时的性能。

**贡献**：比较了基于transformer架构的不同长文档分类方法:即稀疏关注和分层方法。一些设计选择(如稀疏注意方法中的局部注意窗口大小)可以在不牺牲有效性的情况下提高效率，而一些选择(如分层方法中的文档分割策略)会极大地影响有效性。基于transformer的模型可以在MIMIC-III数据集上优于以前最先进的基于CNN的模型。

## 问题陈述和数据集

我们将文档分类模型分为两个组件:*(1)文档编码器，它构建给定文档的向量表示;(2)一个分类器，该分类器预测给定编码向量的单个或多个标签。*我们使用基于transformer的编码器来构建文档表示，然后将编码的文档表示作为分类器的输入。我们使用TANH激活的隐藏层，然后是输出层。输出概率通过应用SIGMOID或者SOFTMAX得到。我们主要在MIMIC-III数据集上进行实验

MIMIC-III包含重症监护病房(ICU)出院摘要，每个摘要都使用ICD-9(国际疾病分类，第九次修订版)层次结构用多个标签-诊断和程序进行注释。根据Mullenbach等人(2018)的研究，我们使用前50个频繁标签进行实验

为了解决一般化的问题，我们还使用了来自其他领域的三个数据集:ECtHR来自法律案件，Hyperpartisan 和20 News 均来自新闻文章。

## 方法

### Sparse（稀少的）-Attention Transformers

Beltagy等人(2020)的Longformer由本地(基于窗口的)注意力和全局注意力组成，这降低了模型的计算复杂性，因此可以部署到处理多达4096个令牌。

BigBird是另一个基于稀疏注意力的Transformer，它使用本地、全局和随机注意力的组合，即所有令牌也会在同一邻域中的令牌之上参加许多随机令牌。这两个模型都是从公共RoBERTa检查点热启动的，并进一步对掩码语言建模进行预训练。据报道，在一系列需要长序列建模的任务中，它们的表现优于RoBERTa。

### Hierarchical Transformers

文本先被分割成段，每个段应该有少于512个token，每个片段可以使用预训练的transformer编码器，将每个片段第一个token的上下文表示与片段位置嵌入相加作为片段表示。片段编码器transformer分为两部分捕获片段之间的关系和上下文片段表示输出列表。

