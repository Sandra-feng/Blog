---
title: Hello World
title: 网页模板 pug 基本语法
categories:
  - 阅读笔记
tags:
  - 学习记录
date: 2023-7-23
updated: 2023-7-23 
toc: true
comments: true
---

# A Survey on Text Classification: From Traditional to Deep Learning论文阅读笔记

论文地址：

[https://dl.acm.org/doi/pdf/10.1145/3495162]: 

中科院三区

![image-20230723102300673](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20230723102300673.png)

文本分类过程流程图。第一个重要步骤是为模型预处理文本数据。

## 传统方法中的文本表示（特征提取）

**BOW**意味着语料库中的所有单词都形成一个映射数组。根据映射数组，句子可以表示为一个向量。向量中的第i个元素表示句子映射数组中第i个单词的频率。向量是句子的BOW。BOW的核心是用字典大小的向量表示每个文本。向量的单个值表示与其在文本中的固有位置相对应的词频。

**N-gram**考虑相邻单词的信息，并通过考虑相邻单词来构建字典。它用于计算句子的概率模型。句子的概率表示为句子中每个单词的联合概率。给定第(N−1)个单词的序列，可以通过预测第N个单词的概率来计算句子的概率。为了简化计算，N-gram模型采用马尔可夫假设。一个单词只与它前面的单词有关。因此，N-gram模型执行大小为n的滑动窗口。通过计数和记录所有片段的出现频率，可以使用记录中相关片段的频率计算句子的概率。

**TF-IDF**使用词频和逆文档频率对文本进行建模。TF是一个词在特定文章中的词频，IDF是包含该词的文章占语料库中文章总数的比例的倒数。TF-IDF是两者的乘积。TF-IDF评估一个词对一组文件或语料库中的一个文档的重要性。一个词的重要性随着它在文档中出现的次数成比例地增加。然而，它在整个语料库中的频率呈反比下降。

**word2vec**利用本地上下文信息获取词向量，词向量是指一个固定长度的实值向量，指定为语料库中任何词的词向量。word2vec使用两个基本模型:CBOW和Skip-gram。前者是在已知当前单词的上下文的前提下预测当前单词。后者是在已知当前单词时预测上下文。

**GloVe**同时具有局部上下文和全局统计特征，对词-词共现矩阵中的非零元素进行训练，它使词向量能够包含尽可能多的语义和语法信息。词向量的构建方法是:首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。最后，根据选择的特征将表示的文本输入到分类器中。



## 传统分类器

**PGM-based方法**。概率图模型(Probabilistic Graphical Models, PGMs)表达图中特征之间的条件依赖关系，如贝叶斯网络[25]。它是概率论和图论的结合。Naïve贝叶斯(NB)：以先验计算后验。朴素贝叶斯公式

**KNN-based**。KNN (k近邻)算法[9]的核心是通过在k个最近的样本上找到样本最多的类别来对未标记的样本进行分类。它是一个简单的分类器，不需要建立模型，并且可以通过快速获得k个最近邻来降低复杂性。

**支持向量机(SVM)**。

**决策树(DT)**[49]是一种有监督的树结构学习方法，反映了分而治之的思想，并且是递归构建的。该算法学习析取表达式，对含有噪声的文本具有鲁棒性。

**Integration-based Methods集成的方法**。传统的集成算法有自举聚合(bootstrap aggregation)，如RF[14]，增强(boosting)，如Adaptive boosting (AdaBoost)[56]和XGBoost[15]，以及叠加。



## 深度学习模型

一些任务：情感分析(SA)、话题标注(TL)、新闻分类(NC)、问答(QA)、对话行为分类(DAC)、自然语言推理(NLI)和关系分类(RC)

递归神经网络**ReNN**

**MLP**多层感知机

循环神经网络**RNN**

自然语言推理(**NLI**)。[181]通过测量每对句子之间的语义相似性来预测一个文本的意义是否可以从另一个文本中推断出来。

**CNN-based**方法。对于文本分类，需要将文本表示为类似于图像表示的向量，并且可以从多个角度过滤文本特征，如图7所示。首先，将输入文本的词向量拼接成一个矩阵。然后将矩阵送入卷积层，该层包含多个不同维数的滤波器。最后，卷积层的结果经过池化层，并将池化结果连接起来，得到文本的最终向量表示。类别由最终向量预测。为了尝试使用CNN进行文本分类任务，Kim引入了一种无偏卷积神经网络模型，称为**TextCNN**[17]

Attention-based Methods。层次注意网络(Hierarchical Attention Network, **HAN**)，通过利用文本中极具信息量的成分来获得更好的可视化效果，如图8所示。HAN包括两个编码器和两个注意层。注意机制让模型对特定的输入给予不同的注意。它首先将关键词聚合成句子向量，然后将重要句子向量聚合成文本向量。基于关注的模型可以了解每个单词和句子对分类判断的贡献程度，通过两个关注层次，有利于应用和分析。**HAPN**[124]用于少镜头文本分类。

**Self-attention**通过在句子中构造K、Q和V矩阵来捕获句子中单词的权重分布，这些矩阵可以捕获对文本分类的长期依赖关系。所有的输出向量都可以并行计算。Lin等[114]在句子表示任务中使用源标记自注意来探索每个标记对整个句子的权重。为了捕获远程依赖关系，双向块自关注网络(**Bi-BloSAN**)[120]对按顺序分割的每个块使用一个块内自关注网络(SAN)，并对输出使用一个块间SAN。

预训练的语言模型[198]可以有效地学习全局语义表示，并显著提高NLP任务，包括文本分类。它通常采用无监督的方法自动挖掘语义知识，然后构建预训练目标，使机器能够学习理解语义。**OpenAI GPT** [199], and **BERT** [19]. **ELMo** [118]，**RoBERTa**[140]，**XLNet**[138]

**图神经网络GNN**-based Methods，文本T = [T1,T2,T3,T4]和文本中的单词X = [x1, x2, x3, x4, x5, x6]，定义为节点，构建成图结构。图节点由粗体黑色边连接，黑色边表示文档-单词边和单词-单词边。每个词-词边的权重通常表示它们在语料库中的共现频率。然后，通过隐藏层表示单词和文本。最后，通过图来预测所有输入文本的标签。**DGCNN**[153]是一种将文本转换为词图的graph-CNN，具有使用CNN模型学习不同层次语义的优势。**TextGCN**为整个数据集构建异构词文本图，并捕获全局词共现信息。**图注意网络(Graph ATtention network, GAT)**[212]通过关注其邻居而采用掩膜自注意层。因此，提出了一些基于gat的模型来计算每个节点的隐藏表示。异构图注意网络(Heterogeneous Graph ATtention networks, **HGAT**)[213]采用双级注意机制，学习当前节点中不同相邻节点和节点类型的重要性。该模型在图上传播信息并捕获关系，以解决半监督短文本分类的语义稀疏性问题。

其他网络：孪生神经网络(twin NN)，虚拟对抗训练(VAT)，强化学习(RL)。

总结：

RNN按顺序计算，不能并行计算。RNN的缺点使其在模型趋于更深入、参数更多的当前趋势下难以成为主流。

CNN通过卷积核从文本向量中提取特征。卷积核捕获的特征的数量与其大小有关，从理论上讲，CNN的深度足以捕捉远距离的特征。由于深层网络参数优化方法的不足，以及池化层造成的位置信息的丢失，深层网络并没有带来明显的改善。与RNN相比，CNN具有并行计算能力，改进后的CNN可以有效地保留位置信息。但是，它对远距离的特征捕获能力较弱。

GNN为文本构建图形。当设计出有效的图结构时，学习到的表示可以更好地捕获结构信息。

Transformer将输入文本视为一个完全连接的图，在边缘上具有注意评分权重。该算法具有并行计算能力，能够高效地通过自注意提取不同单词之间的特征，解决短时记忆问题。然而，Transformer中的注意力机制计算量很大，特别是在处理长序列时。最近提出了一些用于Transformer计算复杂度的改进模型[146,225]。总的来说，Transformer是文本分类的更好选择。

## 数据集

这里只取新闻分类的数据集。20
Newsgroups (20NG) [34], AG News (AG) [93, 234], R8 [235], R52 [235], Sogou News (Sogou)
[136], and so on.

20NG是一个新闻组文本数据集。它有20个类别，每个类别的数量相同，包括18,846个文本。

AG News是一个搜索学术界新闻的搜索引擎，选择了四个最大的课程。它使用每个新闻的标题和描述字段。AG包含120,000个培训文本和7,600个测试文本。

R8和R52是两个子集，它们是Reuters的子集[252]。R8有8个类别，分为2189个测试文件和5485个培训课程。R52有52个类别，分为6532个培训文件和2568个测试文件。

搜狗结合了两个数据集，包括搜狗新闻集和搜狗新闻集。每个文本的标签是URL中的域名。

在评价文本分类模型方面，准确率和F1分数是评价文本分类方法最常用的指标。

在新闻分类任务中**BLSTM-2DCNN**[77]的精确度最高。将双向LSTM (BiLSTM)与二维最大池化相结合。它使用二维卷积来采样矩阵中更有意义的信息，并通过BiLSTM更好地理解上下文。此外，Xue等[189]提出了结合BiLSTM和CNN层的MTNA来解决方面类别分类和方面术语提取任务。

## 未来研究挑战

对于文本分类任务，无论是传统方法还是深度学习方法，数据都是模型性能的关键。本文研究的文本数据主要包括多章节、短文本、跨语言、多标签、少样本文本等。针对这些数据的特点，目前存在的技术挑战如下:

外部知识。众所周知，深度神经网络中输入的有益信息越多，其性能就越好。例如，一个包含常识性知识的问答系统可以回答关于现实世界的问题，并帮助解决信息不完整的问题。因此，增加外部知识(知识库或知识图)[293,294]是提高模型性能的有效途径。现有知识包括概念信息[100,127,204]、常识知识[223]、知识库信息[295,296]、通用知识图[170]等，增强了文本的语义表示。然而，由于输入规模的限制，对于不同的任务，如何以及添加什么仍然是一个挑战。

现有的模型大多是监督模型，过度依赖于大量的标记数据。当样本量过小或出现零样本时，模型的性能将受到显著影响。新的数据集注释需要花费大量的时间。因此，无监督学习和半监督学习具有很大的潜力。此外，特定领域的文本[297,298]，如金融和医学文本，包含许多特定单词或领域专家可理解的俚语，缩写等，这使得现有的预训练单词向量难以处理。

文本表示。在文本预处理阶段，基于向量空间模型的文本表示方法简单有效。但是，该方法会丢失文本的语义信息，因此限制了基于该方法的应用性能。本文提出的基于语义的文本表示方法耗时太长。因此，高效的基于语义的文本表示方法还需要进一步研究。在基于深度学习的文本分类的文本表示中，词嵌入是主要概念，不同语言对表示单元的描述不同。然后，通过模型学习映射规则，将单词表示为向量的形式。因此，如何设计自适应的数据表示方法更有利于深度学习与具体分类任务的结合。

**模型集成**。大多数传统和深度学习模型的结构都被用于文本分类，包括集成方法。RNN需要逐步递归得到全局信息。CNN可以获取局部信息，通过多层叠加可以增加传感场，捕获更全面的上下文信息。注意机制学习句子中单词之间的整体依赖关系。转换器模型依赖于注意机制来建立输入和输出之间的全局依赖关系的深度。因此，设计一个集成模型是值得尝试利用这些模型的。

模型的效率。基于深度学习的文本分类模型非常有效，如cnn、rnn、gnn等。但是，存在许多技术上的限制，如网络层深度、正则化问题、网络学习率等。因此，优化算法，提高模型训练速度还有更广阔的发展空间。

