---
title: 多模态大模型
categories:
  - 自然语言处理
tags:
  - 学习记录
toc: true
date: 2023-11-12 14:04:21
updated: 2023-11-12 14:04:21
comments: true
---

# 多模态大模型

## CLIP: 连接文本和图像的桥梁

CLIP的英文全称是**Contrastive Language-Image Pre-training**，即**一种基于对比文本-图像对的预训练方法或者模型**。CLIP是一种基于对比学习的多模态模型。CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系。有两个 encoder，一个对应图片，一个对应文本，图像和文本经过各自的 encoder 后，通过简单的点乘来代表不同模态的交互（相似性）。

![image-20231112143032743](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112143032743.png)

```python
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter
# extract feature representations of each modality
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]
# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)
# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)
# symmetric loss function
labels = np.arange(n)
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
```



## ALBEF：先对齐后融合

ALBEF 可以说是结合了单流模型与双流模型，其模型由图像编码器（image encoder）、文本编码器（text encoder）和 多模态编码器（multimodal encoder）组成，其中图像编码器采用的是VIT模型，而文本编码器、多模态编码器采用是一个12层的bert模型，其前6层作为文本编码器，后6层作为多模态编码器。图像编码器和文本编码器分别提到的特征可以用对比损失函数进行特征对齐，而后将图像特征和文本特征统一输入到多模态编码器，可以利用常用的MLM和ITM来进行预训练。在 ALBEF 之前，多模态方法通常使用 transformer 的多模态编码器来同时编码视觉和文本特征，由于目标检测器是提前训练好的，因此视觉和文本特征并不是对齐的。图像和文本特征可能距离很远，这使得多模态编码器难以学习到它们之间的交互。为了解决这个问题，ALBEF 通过一个对比损失（也**就是 CLIP 中的 ITC 损失**）在进行多模态交互之前对齐图像和文本数据。

ITC（Image-Text Contrastive）损失函数的核心思想是，模型应该学会将图像和对应的描述性文本映射到特征空间中的相近点，同时将不相匹配的图像和文本映射到远离的点。这通常通过一个对比损失函数来实现，最常用的是InfoNCE损失，它是由一个softmax函数定义的。

在CLIP模型中，一个batch中的每个图像都会和每个文本描述计算一个相似度分数。这些分数被组织成一个相似度矩阵。然后，对于每个图像，其与正确文本描述的相似度分数（即正样本）应该比与任何错误描述的分数（即负样本）都要高。

以下是计算过程：

<img src="C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112144505550.png" alt="image-20231112144505550" style="zoom:67%;" />

**ALBEF预训练任务**分为图文对比（Image-Text Contrastive Learning）、.掩码建模（Masked Language Modeling）、.图文匹配（Image-Text Matching）三个任务。Masked Language Modeling 同时利用图像和上下文文本来预测mask词

![image-20231112145125314](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112145125314.png)

- 下面红色框其实就**类似于 CLIP**，双塔各自编码图像和文本，然后取 CLS 进行对比学习；
- 上面蓝色框就是为了加强不同模态交互用的编码器（前面提到过 CLIP 内积的方式太简单了，这里就是**加强多模态融合**以适配更难的任务）；
- 图像编码器 12 层，文本编码器 6 层，多模态编码器 6 层；其实右侧是将一个 12 层的文本编码器拆成了两部分，这是因为一些研究工作发现**在多模态中需要更强的图像编码器**，进行这样的拆分一定程度上保证了强图像 encoder 和弱文本 encoder，且保证了模型参数不过多的情况下融合图像和文本的信息。

ALBEF总的训练目标就是这三个损失：L_{ITC}+ L_{MLM}+L_{ITM}

**动量蒸馏**的目的是通过在训练过程中引入一个额外的模型，即动量编码器，来增强模型学习到的特征对齐的稳定性和一致性。这个动量编码器是一个慢慢更新的模型版本，它提供了一个更加平滑和稳定的特征表示目标，帮助主模型更好地学习对齐特征。

**动量 (Momentum)**

在这个上下文中，“动量”通常指的是模型参数更新的一种方法，该方法在更新参数时结合了过去的参数。具体来说，在动量方法中，模型的参数更新不仅取决于当前梯度，还取决于之前梯度的累积。这种方法可以看作是物理中动量的概念的应用，有助于模型参数在优化过程中更平滑地移动，从而减少震荡并加速收敛。

在ALBEF的架构中，动量更新通常应用于动量模型（Momentum Model），该模型是主模型的一个影子副本。动量模型的参数是通过应用动量系数（�*β*）对主模型参数的指数加权移动平均得到的。这意味着动量模型的更新慢于主模型，因此可以提供更稳定的表示供主模型在训练过程中学习。

在提供的图中，动量更新部分对应于图中右侧的“momentum update”箭头，它指向动量模型（Momentum Model）。

**蒸馏 (Distillation)**

“蒸馏”在机器学习中通常指知识蒸馏（Knowledge Distillation），这是一种模型压缩技术，其中一个较大或较复杂的模型（称为教师模型）的知识被转移到一个较小或较简单的模型（称为学生模型）。在ALBEF模型的上下文中，蒸馏被用来将动量模型的平滑和稳定的表示传递给主模型，以提高训练的质量和稳定性。

在蒸馏过程中，主模型被训练以模仿动量模型的输出。这样做的原因是动量模型的输出更加平滑和一致，因此它可以作为一个好的目标，引导主模型在特征空间中正确对齐图像和文本。

ALBEF的使用：

```python
# 加载预训练的ALBEF模型
model = load_pretrained_albef_model()

# 对图像进行预处理
preprocessed_images = preprocess_images(raw_images)

# 对文本进行预处理
preprocessed_texts = preprocess_texts(raw_texts)

# 获取图像和文本的特征表示
with torch.no_grad():
    image_features = model.image_encoder(preprocessed_images)
    text_features = model.text_encoder(preprocessed_texts)

# 对于需要融合特征的任务，可以进一步处理
multimodal_features = model.multimodal_encoder(image_features, text_features)

# 使用提取的特征进行下游任务
for task_data in downstream_task_dataset:
    features = extract_features(task_data, model)  # 根据任务提取相应的特征
    predictions = downstream_model(features)  # 可能需要一个额外的下游模型
    # 进行任务相关的处理，例如计算损失，进行反向传播等
```

## BLIP：统一理解和生成的自举多模态模型

![image-20231112154520395](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112154520395.png)

虽然有三个模型，但是大部分参数都是共享的。

- 左一为 Image Encoder（图像编码器）：该组件使用 Vision Transformer（ViT）对图像进行编码，将全局图像特征表示为一个额外的[CLS]标记。
- 左二为 Text Encoder，采用了 BERT 的结构，提取文本特征用于与视觉特征计算 ITC loss。Text Encoder 不与视觉特征计算交叉注意力。
- 左三为 Image-grounded Text Encoder（基于图像的文本编码器），该组件通过在每个 Transformer 块的自注意力（Self-Attention）层和前馈神经网络（Feed Forward Network）之间插入一个交叉注意力（Cross-Attention）层，将视觉信息注入到文本编码中，提取文本特征用于计算 ITM 损失。
- 左四为 Imagegrounded Text Decoder（基于图像的文本解码器），用于进行 LM 语言建模训练（**这里不再是用 MLM 了**），生成与图像相关的文本描述。
- 三个文本编解码器分别为在文本前添加 [CLS]、[Encode]、[Decode] token
- 与 ALBEF 一样，同样采用动量模型为 ITC 生成伪标签；使用 ITC 为 ITM 进行难负例挖掘。

BLIP 的训练流程

![image-20231112155419717](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112155419717.png)

**字幕器 Captioner：**给一张网络图片，生成字幕。它是一个视觉文本解码器，在 COCO 数据集上使用 LM 目标函数微调。给定网络图片  ，Captioner 生成字幕 。

**过滤器 Filter：**过滤掉噪声图文对。它是一个视觉文本编码器，看文本是否与图像匹配，在 COCO 数据集上使用 ITC 和 ITM 目标函数微调。Filter 删除原始 Web 文本 和合成文本 中的嘈杂文本，如果 ITM 头将其预测为与图像不匹配，则认为文本有噪声。

最后，将过滤后的图像-文本对与人工注释对相结合，形成一个新的数据集，作者用它来预训练一个新的模型。以此来过滤掉噪声信息。

## CoCa（Contrastive Captioners）视觉-语言联合表示

1. **模型架构**： CoCa模型通常包括一个图像编码器和一个文本编码器，这两个编码器可以是基于Transformer的架构，如Vision Transformer (ViT) 用于图像编码，BERT或GPT用于文本编码。
2. **多模态学习**： 在预训练阶段，CoCa模型使用大量的图像-文本配对数据来学习这两种模态之间的相互关系。通过对比学习和自回归语言建模相结合的方式，模型被训练来理解图像内容和相应的文本描述。
3. **对比学习**： CoCa模型使用对比学习来确保图像和相应的文本在特征空间中被拉近，而不匹配的图像和文本被推远。这通常涉及到计算正负样本对之间的相似性，并通过对比损失函数来优化。
4. **自回归语言建模**： 为了更好地理解和生成文本，CoCa模型还包括一个自回归语言模型组件，它预测文本序列中的下一个词，以学习文本的内部结构。

![image-20231112170016740](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20231112170016740.png)

使用coca获取多模态表示：

```python
from transformers import CoCaModel, CoCaTokenizer

model = CoCaModel.from_pretrained('coca-base')
tokenizer = CoCaTokenizer.from_pretrained('coca-base')

# 假设你有一个图像处理函数和文本处理函数
processed_images = image_preprocessing(your_images)
processed_texts = tokenizer(your_texts, padding=True, truncation=True, return_tensors="pt")

# 获取多模态表示
outputs = model(input_ids=processed_texts['input_ids'], pixel_values=processed_images)
multimodal_representation = outputs.pooler_output

# 使用多模态表示进行下游任务
downstream_model = SomeDownstreamTaskModel()
predictions = downstream_model(multimodal_representation)

# 微调模型
outputs = model(input_ids=processed_texts['input_ids'], pixel_values=processed_images, labels=your_labels)
loss = outputs.loss
loss.backward()
optimizer.step()


```

它是使用**对比损失**和**文本生成损失**进行训练，也就是使用了 ITC 和 LM loss；这里没有使用 ITM loss，**减少了模型参数每次迭代所需前向传播的次数**，从而降低了训练时间。

